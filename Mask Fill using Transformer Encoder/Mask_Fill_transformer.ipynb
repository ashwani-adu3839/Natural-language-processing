{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask_Fill_transformer",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlBH21VLU0b-"
      },
      "source": [
        "!pip install tensorflow_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYR-Ia3w4Bc5"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from unicodedata import normalize\n",
        "import tensorflow_text as tf_text\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjFzP59L4Bc9"
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,   as_supervised=True)\n",
        "\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kojKAr6U4Bc-"
      },
      "source": [
        "# Process the data file\n",
        "def process_pairs(line):\n",
        "    line = tf.strings.lower(line)\n",
        "    line = tf_text.normalize_utf8(line, 'NFKD')\n",
        "    line = tf.strings.regex_replace(line, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\")\n",
        "    return line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs91pzAdVunj"
      },
      "source": [
        "dataset_train = train_dataset.map(lambda x, y: (process_pairs(x)))\n",
        "dataset_test = test_dataset.map(lambda x, y: (process_pairs(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxR--eHFcxUx",
        "outputId": "fe3dba5e-e82d-4947-fe18-fc344c4fdc10"
      },
      "source": [
        "for x in dataset_train:\n",
        "   print(x)\n",
        "   break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "fRhSReOxlr7y",
        "outputId": "1c70406b-6843-477c-a14d-7df2d658b723"
      },
      "source": [
        "plt.hist([len(s.numpy()) for s in dataset_train],log=True, bins=100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZNv4mSu7ix"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGt34QOE3Dd9",
        "outputId": "5348bc0f-e0dc-4e5c-d800-d8ac2ae40aeb"
      },
      "source": [
        "custom_standardization('I have watched this [mask] and it was good')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'i have watched this [mask] and it was good'>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quTmhO2cmvgX"
      },
      "source": [
        "max_length_word = 250\n",
        "vocab_size = 30000\n",
        "\n",
        "lang_tokenizer = tf.keras.layers.TextVectorization(max_tokens=vocab_size, output_mode=\"int\", \n",
        "                                                   output_sequence_length=max_length_word, standardize=custom_standardization)\n",
        "lang_tokenizer.adapt(dataset_train)\n",
        "vocab = lang_tokenizer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYLOauJdtkuK"
      },
      "source": [
        "vocab = vocab[2 : vocab_size - 1] + [\"[mask]\"]\n",
        "lang_tokenizer.set_vocabulary(vocab)\n",
        "\n",
        "train_padded = dataset_train.map(lang_tokenizer)\n",
        "testing_padded = dataset_test.map(lang_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt-19g1TtnA8"
      },
      "source": [
        "id2token = dict(enumerate(lang_tokenizer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS6bOvt105kb",
        "outputId": "7415f352-6c39-473b-af71-39fba4e898ba"
      },
      "source": [
        "mask_index = token2id[\"[mask]\"]\n",
        "mask_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29999"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCRU2GZttWK1"
      },
      "source": [
        "# Mask the words in data\n",
        "def masking_word(train_padded):\n",
        "\n",
        "    inp_mask = tf.experimental.numpy.random.randn(max_length_word) < 0.15\n",
        "    mask = inp_mask & (np.random.rand(max_length_word) < 0.15)#    mask = np.random.randn(train_padded.shape[0]) < 1\n",
        "    y_l = tf.where(train_padded > 1,tf.cast(mask, tf.int64), train_padded)\n",
        "    y_train = train_padded-y_l*train_padded+y_l*mask_index\n",
        "    return y_train\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9RUnaeRu9Hr"
      },
      "source": [
        "y_train = train_padded.map(lambda x: (masking_word(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az-q-jH6t_wb"
      },
      "source": [
        "y_test = testing_padded.map(lambda x: (masking_word(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18xYUGpELb7f"
      },
      "source": [
        "def data_to_numpy(data, label):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    \n",
        "    for sentence, y_label in zip(data, label):\n",
        "\n",
        "        sentences.append(sentence.numpy())\n",
        "        labels.append(y_label.numpy())\n",
        "\n",
        "    labels_np = np.array(labels)\n",
        "    sentences = np.array(sentences)\n",
        "    return sentences, labels_np\n",
        "training_sentences, train_mask = data_to_numpy(train_padded, y_train)\n",
        "testing_sentences, testing_mask = data_to_numpy(testing_padded, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKk1rvsNLzz_",
        "outputId": "18c6009b-10bb-4e08-98b6-a5ae7adfa8ab"
      },
      "source": [
        "testing_sentences, val_sentences, testing_mask, val_mask = train_test_split(testing_sentences, testing_mask, test_size=5000, random_state=42)\n",
        "len(testing_sentences), len(val_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8HetgfBvzZJ"
      },
      "source": [
        "BUFFER_SIZE = len(training_sentences)\n",
        "BATCH_SIZE = 16\n",
        "input_vocab_size = vocab_size\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((training_sentences, train_mask)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRU21V6pwK84"
      },
      "source": [
        "BUFFER_SIZE_TEST = len(testing_sentences)\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((testing_sentences, testing_mask)).shuffle(BUFFER_SIZE_TEST)\n",
        "dataset_test = dataset_test.batch(BATCH_SIZE, drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4JFCIcgwM0W"
      },
      "source": [
        "BUFFER_SIZE_VAL = len(val_sentences)\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((val_sentences, val_mask)).shuffle(BUFFER_SIZE_VAL)\n",
        "dataset_val = dataset_val.batch(BATCH_SIZE, drop_remainder=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7YZZZKXwY34"
      },
      "source": [
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(self.head_dim)\n",
        "        self.wk = tf.keras.layers.Dense(self.head_dim)\n",
        "        self.wv = tf.keras.layers.Dense(self.head_dim)\n",
        "        self.dense = tf.keras.layers.Dense(self.embed_size)\n",
        "\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        # Get number of training examples\n",
        "        batch_size = q.shape[0]\n",
        "\n",
        "        seq_len_v, seq_len_k, seq_len_q = v.shape[1], k.shape[1], q.shape[1]\n",
        "\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        v = tf.reshape(v,(batch_size,seq_len_v, self.heads, self.head_dim))\n",
        "        k = tf.reshape(k,(batch_size,seq_len_k, self.heads, self.head_dim))\n",
        "        q = tf.reshape(q,(batch_size,seq_len_q, self.heads, self.head_dim))\n",
        "\n",
        "\n",
        "        values = self.wv(v)  # (batch_size, value_len, heads, head_dim)\n",
        "        keys = self.wk(k)  # (batch_size, key_len, heads, head_dim)\n",
        "        queries = self.wq(q)  # (batch_size, query_len, heads, heads_dim)\n",
        "\n",
        "        # Einsum does matrix mult. for query*keys for each training example\n",
        "\n",
        "        attention = tf.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
        "        # queries shape: (batch_size, seq_len_q, heads, heads_dim),\n",
        "        # keys shape: (batch_size, seq_len_k, heads, heads_dim)\n",
        "        # attention: (batch_size, heads, seq_len_q, seq_len_k)\n",
        "\n",
        "         # scale matmul_qk\n",
        "        dk = tf.cast(tf.shape(keys)[1], tf.float32)\n",
        "        scaled_attention_logits = attention / tf.math.sqrt(dk)\n",
        "\n",
        "        # Mask padded indices so their weights become 0\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # Normalize energy values similarly to seq2seq + attention\n",
        "        # so that they sum to 1. Also divide by scaling factor for\n",
        "        # better stability\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        # attention shape: (batch_size, heads, seq_len_q, seq_len_k)\n",
        "\n",
        "\n",
        "\n",
        "        out = tf.einsum(\"nhql,nlhd->nqhd\", attention_weights, values)\n",
        "        out = tf.reshape(out, (batch_size, seq_len_q, -1))\n",
        "        # attention shape: (batch_size, heads, seq_len_q, key_len)\n",
        "        # values shape: (batch_size, seq_len_v, heads, heads_dim)\n",
        "        # out after matrix multiply: (batch_size, seq_len_q, heads, head_dim), then\n",
        "        # we reshape and flatten the last two dimensions.\n",
        "\n",
        "        out = self.dense(out)\n",
        "        # Linear layer doesn't modify the shape, final shape will be\n",
        "        # (N, query_len, embed_size)\n",
        "\n",
        "        return out, attention_weights\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embed_size, heads, forward_expansion, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.self_attens= SelfAttention(embed_size, heads)\n",
        "    self.ffn = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(forward_expansion, activation='relu'),  # (batch_size, seq_len, forward_expansion)\n",
        "      tf.keras.layers.Dense(embed_size)  # (batch_size, seq_len, d_model)\n",
        "       ])\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.self_attens(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, num_layers, embed_size,  heads, forward_expansion, input_vocab_size, max_length, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.num_layers = num_layers\n",
        "        self.max_length = max_length\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, embed_size)\n",
        "\n",
        "        self.Layerss =[EncoderLayer(embed_size, heads, forward_expansion, rate)\n",
        "                       for _ in range(self.num_layers)]\n",
        "                      \n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        self.last_layer = tf.keras.layers.Dense(input_vocab_size, activation = 'softmax')\n",
        "\n",
        "    def positional_encoding(self):\n",
        "\n",
        "        pos = np.arange(self.max_length)[:, np.newaxis]\n",
        "        i = np.arange(self.embed_size)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(self.embed_size))\n",
        "\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # apply sin to even indices in the array; 2i\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "        # apply cos to odd indices in the array; 2i+1\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "                                                         \n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.embed_size, tf.float32))\n",
        "        x += self.positional_encoding()[:, :seq_len, :]\n",
        "\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "           x = self.Layerss[i](x, training, mask)\n",
        "        x = self.last_layer(x)\n",
        "        return x  # (batch_size, input_seq_len, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7_ebrpkw76C"
      },
      "source": [
        "num_layers = 4\n",
        "embed_size = 64\n",
        "heads = 4\n",
        "forward_expansion  = embed_size*2\n",
        "input_vocab_size = vocab_size\n",
        "max_length = 1000 \n",
        "rate = 0.1\n",
        "model = Encoder(num_layers,  embed_size, heads, forward_expansion,input_vocab_size, max_length, rate=rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfrXC7NR3XYB"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whu_j8GmyZJz",
        "outputId": "7abbfb92-153c-4b7e-a03a-8b9a15563bcd"
      },
      "source": [
        "for x, y in dataset:\n",
        "    mask = create_padding_mask(x)\n",
        "    outputx = model(x, training=True, mask=mask)\n",
        "    print(outputx.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 250, 30000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8l-yGCNzRF4"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4xQQaoTzYUb"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(inp,training = True, mask = enc_padding_mask)\n",
        "      loss = loss_object(tar, logits)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_accuracy.update_state(tar, logits)\n",
        "    return loss\n",
        "def val_step(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    logits = model(inp, False, enc_padding_mask)\n",
        "    loss = loss_object(tar, logits)\n",
        "    val_accuracy.update_state(tar, logits)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9mLMB0pzpYA",
        "outputId": "3e5cd5fb-a56c-4531-a528-bb192232010b"
      },
      "source": [
        "EPOCHS =10\n",
        "history = {\n",
        "  \"epoch\": [],\n",
        "  \"loss\": [],\n",
        "  \"Accuracy\" :[],\n",
        "  \"Val_loss\": [],\n",
        "  \"Val_Accuracy\":[]  \n",
        "}\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  total_loss = 0\n",
        "  val_loss = 0\n",
        "#  train_loss.reset_states()\n",
        "  for (batch, (inp, tar)) in enumerate(dataset):\n",
        "    batch_loss = train_step(inp, tar)\n",
        "    total_loss = total_loss+batch_loss\n",
        "  history['epoch'].append(epoch)\n",
        "  history['loss'].append(total_loss/(batch+1))\n",
        "  history['Accuracy'].append(train_accuracy.result())\n",
        "\n",
        "  # TEST LOOP\n",
        "  for (batch, (inp, tar)) in enumerate(dataset_val):\n",
        "    batch_loss = val_step(inp, tar)\n",
        "    val_loss = val_loss+batch_loss\n",
        "  history['Val_loss'].append(val_loss/(batch+1))\n",
        "  history['Val_Accuracy'].append(val_accuracy.result())\n",
        "\n",
        "  if (epoch+1) % 1 == 0:    \n",
        "        print(f'Epoch {epoch+1}, Loss: {total_loss/(batch+1):.4f}, Accuracy: {train_accuracy.result():.4f}, Validation Loss: {val_loss/(batch+1):.4f} Validation Accuracy: {val_accuracy.result():.4f}') \n",
        "        print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        "  train_accuracy.reset_states()\n",
        "  val_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 29.5543, Accuracy: 0.3197, Validation Loss: 3.4473 Validation Accuracy: 0.5574\n",
            "Time taken for 1 epoch: 189.62 secs\n",
            "\n",
            "Epoch 2, Loss: 12.2878, Accuracy: 0.6893, Validation Loss: 1.7344 Validation Accuracy: 0.7726\n",
            "Time taken for 1 epoch: 190.24 secs\n",
            "\n",
            "Epoch 3, Loss: 7.1662, Accuracy: 0.8069, Validation Loss: 1.2506 Validation Accuracy: 0.8267\n",
            "Time taken for 1 epoch: 191.63 secs\n",
            "\n",
            "Epoch 4, Loss: 5.0411, Accuracy: 0.8548, Validation Loss: 1.0089 Validation Accuracy: 0.8573\n",
            "Time taken for 1 epoch: 191.55 secs\n",
            "\n",
            "Epoch 5, Loss: 3.7932, Accuracy: 0.8838, Validation Loss: 0.8646 Validation Accuracy: 0.8770\n",
            "Time taken for 1 epoch: 191.79 secs\n",
            "\n",
            "Epoch 6, Loss: 2.9448, Accuracy: 0.9037, Validation Loss: 0.7757 Validation Accuracy: 0.8934\n",
            "Time taken for 1 epoch: 191.81 secs\n",
            "\n",
            "Epoch 7, Loss: 2.3334, Accuracy: 0.9182, Validation Loss: 0.7257 Validation Accuracy: 0.9014\n",
            "Time taken for 1 epoch: 191.64 secs\n",
            "\n",
            "Epoch 8, Loss: 1.8778, Accuracy: 0.9291, Validation Loss: 0.6938 Validation Accuracy: 0.9134\n",
            "Time taken for 1 epoch: 191.78 secs\n",
            "\n",
            "Epoch 9, Loss: 1.5310, Accuracy: 0.9373, Validation Loss: 0.6980 Validation Accuracy: 0.9153\n",
            "Time taken for 1 epoch: 191.80 secs\n",
            "\n",
            "Epoch 10, Loss: 1.2595, Accuracy: 0.9440, Validation Loss: 0.6894 Validation Accuracy: 0.9251\n",
            "Time taken for 1 epoch: 191.68 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "OGbQJnhrfDA4",
        "outputId": "7135f118-399d-4edd-91f6-261506e9b0f9"
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history[\"epoch\"],history[\"loss\"], label='loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgTnHd3_9yDN"
      },
      "source": [
        "def masked_sentence(sentenceX):\n",
        "    sentence = process_pairs(sentenceX)\n",
        "    sentence = sentence.numpy()\n",
        "    print(sentence)\n",
        "    inputs = lang_tokenizer([sentence])#tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_word, padding='post')\n",
        "    enc_padding_mask = create_padding_mask(inputs)\n",
        "    logits = model(inputs, False, enc_padding_mask)\n",
        "    prob = tf.argmax(logits, axis=-1).numpy()\n",
        "    print(logits.shape)\n",
        "    result = ''\n",
        "    for p in prob[0]:\n",
        "      if p==0:\n",
        "        break\n",
        "      result += id2token[p] + ' ' \n",
        "    print(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihO61btf_CwG",
        "outputId": "3437e112-da94-4fc3-9aa7-bbd5b6b61f9e"
      },
      "source": [
        "masked_sentence(u'I have watched this [mask] and he was [mask]')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'i have watched this [mask] and he was [mask]'\n",
            "i have watched this astronaut and he was monitors \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUMXx7cRA8V8"
      },
      "source": [
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK7NyOW4fJi0"
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def test_step(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    logits = model(inp, False, enc_padding_mask)\n",
        "    loss = loss_object(tar, logits)\n",
        "    test_accuracy.update_state(tar, logits)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP00QLum_eIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7d07cb-c8a7-431d-8b07-94f88a60c286"
      },
      "source": [
        "start = time.time()\n",
        "test_loss = 0\n",
        "for (batch, (inp, tar)) in enumerate(dataset_test):\n",
        "   batch_loss = test_step(inp, tar)\n",
        "   test_loss = test_loss+batch_loss\n",
        "print('Accuracy of model on Test : ', test_accuracy.result().numpy())\n",
        "print(f'Loss of model on Test : {test_loss.numpy()/(batch+1):.4f}')\n",
        "print(f'Time taken for epoch: {time.time() - start:.2f} secs\\n')\n",
        "test_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model on Test :  0.925187\n",
            "Loss of model on Test : 0.6904\n",
            "Time taken for 1 epoch: 89.39 secs\n",
            "\n"
          ]
        }
      ]
    }
  ]
}